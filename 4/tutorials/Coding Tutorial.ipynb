{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and loading models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Coding tutorials\n",
    " #### [1. Saving and loading model weights](#coding_tutorial_1)\n",
    " #### [2. Model saving criteria](#coding_tutorial_2)\n",
    " #### [3. Saving the entire model](#coding_tutorial_3)\n",
    " #### [4. Loading pre-trained Keras models](#coding_tutorial_4)\n",
    " #### [5. Tensorflow Hub modules](#coding_tutorial_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_1\"></a>\n",
    "## Saving and loading model weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and inspect CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset consists of, in total, 60000 color images, each with one of 10 labels: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck. For an introduction and a download, see [this link](https://www.cs.toronto.edu/~kriz/cifar.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Import the CIFAR-10 dataset and rescale the pixel values\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "# Use smaller subset -- speeds things up\n",
    "x_train = x_train[:10000]\n",
    "y_train = y_train[:10000]\n",
    "x_test = x_test[:1000]\n",
    "y_test = y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 10 CIFAR-10 images\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 10, figsize=(10, 1))\n",
    "for i in range(10):\n",
    "    ax[i].set_axis_off()\n",
    "    ax[i].imshow(x_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce two useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce function to test model accuracy\n",
    "\n",
    "def get_test_accuracy(model, x_test, y_test):\n",
    "    test_loss, test_acc = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "    print('accuracy: {acc:0.3f}'.format(acc=test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Introduce function that creates a new instance of a simple CNN\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "def get_new_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(filters=16, input_shape=(32, 32, 3), kernel_size=(3, 3), \n",
    "               activation='relu', name='conv_1'),\n",
    "        Conv2D(filters=8, kernel_size=(3, 3), activation='relu', name='conv_2'),\n",
    "        MaxPooling2D(pool_size=(4, 4), name='pool_1'),\n",
    "        Flatten(name='flatten'),\n",
    "        Dense(units=32, activation='relu', name='dense_1'),\n",
    "        Dense(units=10, activation='softmax', name='dense_2')\n",
    "    ])\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create simple convolutional neural network classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_1 (Conv2D)              (None, 30, 30, 16)        448       \n",
      "_________________________________________________________________\n",
      "conv_2 (Conv2D)              (None, 28, 28, 8)         1160      \n",
      "_________________________________________________________________\n",
      "pool_1 (MaxPooling2D)        (None, 7, 7, 8)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 392)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                12576     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 14,514\n",
      "Trainable params: 14,514\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the model and show model summary\n",
    "\n",
    "model = get_new_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.087\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy of the untrained model, around 10% (random)\n",
    "\n",
    "get_test_accuracy(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model with checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object\n",
    "\n",
    "checkpoint_path = 'ModelCheckpoints/checkpoint'\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             frequency = 'epoch',\n",
    "                             save_weight_only = True,\n",
    "                             verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples\n",
      "Epoch 1/3\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 1.9344 - accuracy: 0.2897\n",
      "Epoch 00001: saving model to ModelCheckpoints/checkpoint\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ModelCheckpoints/checkpoint/assets\n",
      "10000/10000 [==============================] - 54s 5ms/sample - loss: 1.9340 - accuracy: 0.2895\n",
      "Epoch 2/3\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 1.6064 - accuracy: 0.4117\n",
      "Epoch 00002: saving model to ModelCheckpoints/checkpoint\n",
      "INFO:tensorflow:Assets written to: ModelCheckpoints/checkpoint/assets\n",
      "10000/10000 [==============================] - 52s 5ms/sample - loss: 1.6066 - accuracy: 0.4116\n",
      "Epoch 3/3\n",
      " 9984/10000 [============================>.] - ETA: 0s - loss: 1.4925 - accuracy: 0.4607\n",
      "Epoch 00003: saving model to ModelCheckpoints/checkpoint\n",
      "INFO:tensorflow:Assets written to: ModelCheckpoints/checkpoint/assets\n",
      "10000/10000 [==============================] - 52s 5ms/sample - loss: 1.4922 - accuracy: 0.4611\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x75b1b2869748>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model, with simple checkpoint which saves (and overwrites) model weights every epoch\n",
    "model.fit(x_train,y_train,epochs =3,callbacks =[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = 'model_checkpoints_5000'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 904K\r\n",
      "-rw-r--r-- 1 jovyan users   93 May 31 11:23 checkpoint\r\n",
      "-rw-r--r-- 1 jovyan users 174K May 31 11:22 checkpoint_01_0156.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K May 31 11:22 checkpoint_01_0156.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K May 31 11:22 checkpoint_02_0000.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K May 31 11:22 checkpoint_02_0000.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K May 31 11:23 checkpoint_02_0157.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K May 31 11:23 checkpoint_02_0157.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K May 31 11:23 checkpoint_03_0001.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K May 31 11:23 checkpoint_03_0001.index\r\n",
      "-rw-r--r-- 1 jovyan users 174K May 31 11:23 checkpoint_03_0158.data-00000-of-00001\r\n",
      "-rw-r--r-- 1 jovyan users 2.0K May 31 11:23 checkpoint_03_0158.index\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lh model_checkpoints_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_dir = 'model_checkpoints'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n"
     ]
    }
   ],
   "source": [
    "# Have a look at what the checkpoint creates\n",
    "\n",
    "! ls -lh model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.465\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the performance of the trained model\n",
    "\n",
    "get_test_accuracy(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new model, load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.085\n"
     ]
    }
   ],
   "source": [
    "# Create a new instance of the (initialised) model, accuracy around 10% again\n",
    "model = get_new_model()\n",
    "get_test_accuracy(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file read failed: time = Fri May 31 11:07:38 2024\n, filename = 'ModelCheckpoints/checkpoint', file descriptor = 45, errno = 21, error message = 'Is a directory', buf = 0x7fff3ba3b4b0, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-55cdd6924af1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load weights -- accuracy is the same as the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mget_test_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m    179\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    180\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 181\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name)\u001b[0m\n\u001b[1;32m   1169\u001b[0m           'first, then load the weights.')\n\u001b[1;32m   1170\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_weights_created\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'layer_names'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattrs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'model_weights'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    393\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file read failed: time = Fri May 31 11:07:38 2024\n, filename = 'ModelCheckpoints/checkpoint', file descriptor = 45, errno = 21, error message = 'Is a directory', buf = 0x7fff3ba3b4b0, total read size = 8, bytes this sub-read = 8, bytes actually read = 18446744073709551615, offset = 0)"
     ]
    }
   ],
   "source": [
    "# Load weights -- accuracy is the same as the trained model\n",
    "model.load_weights(checkpoint_path)\n",
    "get_test_accuracy(model,x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_2\"></a>\n",
    "## Model saving criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create more customised checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object with epoch and batch details\n",
    "\n",
    "checkpoint_5000_path = 'model_checkpoints_5000/checkpoint_{epoch:02d}_{batch:04d}'\n",
    "checkpoint_5000 = ModelCheckpoint(filepath=checkpoint_5000_path,\n",
    "                                  save_weights_only=True,\n",
    "                                  save_freq=5000,\n",
    "                                  verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10000 samples, validate on 1000 samples\n",
      "Epoch 1/3\n",
      " 4992/10000 [=============>................] - ETA: 27s - loss: 2.2030 - accuracy: 0.1849\n",
      "Epoch 00001: saving model to model_checkpoints_5000/checkpoint_01_0156\n",
      "10000/10000 [==============================] - 55s 6ms/sample - loss: 2.0418 - accuracy: 0.2513 - val_loss: 1.7166 - val_accuracy: 0.3710\n",
      "Epoch 2/3\n",
      "\n",
      "Epoch 00002: saving model to model_checkpoints_5000/checkpoint_02_0000\n",
      " 5024/10000 [==============>...............] - ETA: 23s - loss: 1.6720 - accuracy: 0.3897\n",
      "Epoch 00002: saving model to model_checkpoints_5000/checkpoint_02_0157\n",
      "10000/10000 [==============================] - 49s 5ms/sample - loss: 1.6078 - accuracy: 0.4167 - val_loss: 1.5157 - val_accuracy: 0.4470\n",
      "Epoch 3/3\n",
      "   32/10000 [..............................] - ETA: 30s - loss: 1.4757 - accuracy: 0.5312\n",
      "Epoch 00003: saving model to model_checkpoints_5000/checkpoint_03_0001\n",
      " 5056/10000 [==============>...............] - ETA: 23s - loss: 1.4788 - accuracy: 0.4705\n",
      "Epoch 00003: saving model to model_checkpoints_5000/checkpoint_03_0158\n",
      "10000/10000 [==============================] - 49s 5ms/sample - loss: 1.4438 - accuracy: 0.4773 - val_loss: 1.4826 - val_accuracy: 0.4660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x75b2592599e8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and fit model with checkpoint\n",
    "model = get_new_model()\n",
    "model.fit(x_train,y_train,epochs =3,validation_data = (x_test,y_test),callbacks =[checkpoint_5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at what the checkpoint creates\n",
    "\n",
    "! ls -lh model_checkpoints_5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with model saving criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tiny training and test set -- will overfit!\n",
    "\n",
    "x_train = x_train[:100]\n",
    "y_train = y_train[:100]\n",
    "x_test = x_test[:100]\n",
    "y_test = y_test[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new instance of untrained model\n",
    "\n",
    "model = get_new_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object which monitors the validation accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model and save only the weights with the highest validation accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and testing curves\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(history.history)\n",
    "df.plot(y=['accuracy', 'val_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the checkpoint directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new model with the saved weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r model_checkpoints_5000 model_checkpoints_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_3\"></a>\n",
    "## Saving the entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create checkpoint that saves whole model, not just weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tensorflow checkpoint object\n",
    "checkpoint_path = 'ModelCheckpoints'\n",
    "checkpoint = ModelCheckpoint(filepath = checkpoint_path,\n",
    "                             frequency = 'epoch',\n",
    "                             save_weight_only = False,\n",
    "                             verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit model with checkpoint\n",
    "model = get_new_model()\n",
    "model.fit(x_train,y_train,epochs =3,validation_data = (x_test,y_test),callbacks =[checkpoint])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect what the checkpoint has created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have a look at what the checkpoint creates\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter variables directory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the model's test accuracy\n",
    "get_test_accuracy(model,x_test,y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from scratch\n",
    "\n",
    "model = load_model(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the .h5 format to save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in .h5 format\n",
    "\n",
    "model.save(my_model.h5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect .h5 file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload model from scratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -r model_checkpoints\n",
    "! rm my_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_4\"></a>\n",
    "## Loading pre-trained Keras models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and build Keras ResNet50 model\n",
    "\n",
    "Today we'll be using the ResNet50 model designed by a team at Microsoft Research, available through Keras applications. Please see the description on the [Keras applications page](https://keras.io/applications/#resnet) for details. If you continue using it, please cite it properly! The paper it comes from is:\n",
    "\n",
    "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun. \"Deep Residual Learning for Image Recognition\", 2015.\n",
    "\n",
    "This model takes a long time to download on the Coursera platform, so it is pre-downloaded in your workspace and saved in Keras HDF5 format. If you want to import it on your personal machine, use the following code:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "model = ResNet50(weights='imagenet')\n",
    "```\n",
    "\n",
    "In this coding tutorial, you will instead load the model directly from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: models/Keras_Resnet50.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-21f95f34bcc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build Keras ResNet50 model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/Keras_Resnet50.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: models/Keras_Resnet50.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "# Build Keras ResNet50 model\n",
    "\n",
    "model = load_model('models/Keras_Resnet50.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and preprocess 3 sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 3 sample ImageNet images\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "lemon_img = load_img('data/lemon.jpg', target_size=(224, 224))\n",
    "viaduct_img = load_img('data/viaduct.jpg', target_size=(224, 224))\n",
    "water_tower_img = load_img('data/water_tower.jpg', target_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use ResNet50 model to classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: presents top 5 predictions and probabilities\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_5_predictions(img):\n",
    "    x = img_to_array(img)[np.newaxis, ...]\n",
    "    x = preprocess_input(x)\n",
    "    preds = decode_predictions(model.predict(x), top=5)\n",
    "    top_preds = pd.DataFrame(columns=['prediction', 'probability'],\n",
    "                             index=np.arange(5)+1)\n",
    "    for i in range(5):\n",
    "        top_preds.loc[i+1, 'prediction'] = preds[0][i][1]\n",
    "        top_preds.loc[i+1, 'probability'] = preds[0][i][2] \n",
    "    return top_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 1: lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=224x224 at 0x7BE88C29C898>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display image\n",
    "\n",
    "lemon_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a02778ee97af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Display top 5 predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mget_top_5_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemon_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-1372ef8acab2>\u001b[0m in \u001b[0;36mget_top_5_predictions\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     top_preds = pd.DataFrame(columns=['prediction', 'probability'],\n\u001b[1;32m     13\u001b[0m                              index=np.arange(5)+1)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# Display top 5 predictions\n",
    "\n",
    "get_top_5_predictions(lemon_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 2: viaduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 3: water tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display top 5 predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "<a id=\"coding_tutorial_5\"></a>\n",
    "## Tensorflow Hub modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and build Tensorflow Hub MobileNet v1 model\n",
    "\n",
    "Today we'll be using Google's MobileNet v1 model, available on Tensorflow Hub. Please see the description on the [Tensorflow Hub page](https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/classification/4) for details on it's architecture, how it's trained, and the reference. If you continue using it, please cite it properly! The paper it comes from is:\n",
    "\n",
    "Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam: \"MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications\", 2017.\n",
    "\n",
    "This model takes a long time to download on the Coursera platform, so it is pre-downloaded in your workspace and saved in Tensorflow SavedModel format. If you want to import it on your personal machine, use the following code:\n",
    "\n",
    "```python\n",
    "module_url = \"https://tfhub.dev/google/imagenet/mobilenet_v1_050_160/classification/4\"\n",
    "model = Sequential([hub.KerasLayer(module_url)])\n",
    "model.build(input_shape=[None, 160, 160, 3])\n",
    "```\n",
    "\n",
    "In this coding tutorial, you will instead load the model directly from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Google's Mobilenet v1 model\n",
    "\n",
    "module = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use MobileNet model to classify images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import and preprocess 3 sample ImageNet images\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "lemon_img = load_img(\"data/lemon.jpg\", target_size=(160, 160))\n",
    "viaduct_img = load_img(\"data/viaduct.jpg\", target_size=(160, 160))\n",
    "water_tower_img = load_img(\"data/water_tower.jpg\", target_size=(160, 160))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in categories text file\n",
    "\n",
    "with open('data/imagenet_categories.txt') as txt_file:\n",
    "    categories = txt_file.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful function: presents top 5 predictions\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def get_top_5_predictions(img):\n",
    "    x = img_to_array(img)[np.newaxis, ...] / 255.0\n",
    "    preds = model.predict(x)\n",
    "    top_preds = pd.DataFrame(columns=['prediction'],\n",
    "                             index=np.arange(5)+1)\n",
    "    sorted_index = np.argsort(-preds[0])\n",
    "    for i in range(5):\n",
    "        ith_pred = categories[sorted_index[i]]\n",
    "        top_preds.loc[i+1, 'prediction'] = ith_pred\n",
    "            \n",
    "    return top_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 1: lemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 2: viaduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Image 3: water tower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
